{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209c9d5-b78d-436a-8b97-624fdbc53072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "merge_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "merge_model = merge_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1d2d2",
   "metadata": {},
   "source": [
    "## Pipeline to generate a single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db4997-6257-4dc0-b747-39859cff5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "df = load_dataset(\"cnn_dailymail\",  \"1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28671f-e517-4d57-956f-baa8228ecd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import Pipeline\n",
    "from torch import Tensor\n",
    "from adpated_forward_call import run_merge\n",
    "\n",
    "class MyPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self,\n",
    "                             **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        inputs = self.tokenizer(inputs, return_tensors = \"pt\", max_length = self.sl, truncation = True)\n",
    "        model_input = Tensor(inputs[\"input_ids\"][:,:self.sl])\n",
    "        return {\"model_input\": model_input}\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        logits, length = run_merge(tokens = model_inputs[\"model_input\"], \n",
    "                                   cutoff = self.lc, \n",
    "                                   starting_tokens=self.starting_tokens,\n",
    "                                   max_tokens_before_keeping_end=self.max_tokens_before_keeping_end,\n",
    "                                   ending_tokens = self.ending_tokens\n",
    "                                   self.model)\n",
    "        return {\"logits\" : logits, \"length\" : length}\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        top_5_l, top_5_i = torch.topk(model_outputs[\"logits\"], k=5, dim=-1)\n",
    "        top_5_l = top_5_l[0,-1,:]\n",
    "        top_5_i = top_5_i[0,-1,:]\n",
    "        return {\"top_5_l\" : top_5_l.numpy(),\n",
    "                \"top_5_i\" : top_5_i.numpy(),\n",
    "                \"length\" : model_outputs[\"length\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6976d0-743c-4f18-a0a1-6401fab73761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "check_df = df[\"train\"].shuffle().select(range(5000))\n",
    "\n",
    "k_df = KeyDataset(check_df, \"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = MyPipeline(model = merge_model, \n",
    "                      tokenizer = tokenizer,\n",
    "                      device = 0,\n",
    "                      num_workers = 8)\n",
    "\n",
    "pipeline.sl = 516 # Change this to the max sequence length you want\n",
    "pipeline.lc = 8 # Change this to apply the merging at the layer you wish\n",
    "pipeline.starting_tokens = 0\n",
    "pipeline.max_tokens_before_keeping_end = 100\n",
    "pipeline.ending_tokens = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7e2cb-365f-43d3-b9d1-3f262f884858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in pipeline(k_df):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae12a63",
   "metadata": {},
   "source": [
    "## Pipeline to generate with merged tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d69fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from OptimizedMistral import OptimizedInferenceMistral\n",
    "\n",
    "opt_model = OptimizedInferenceMistral.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16)\n",
    "opt_model.to(device)\n",
    "opt_model.cutoff = 20 # Layer indice where you apply merging\n",
    "\n",
    "opt_model.max_tokens_before_keeping_end = 64 # Tokens limit to reach before not merging the last tokens defined under\n",
    "opt_model.ending_tokens = 16 # How many tokens to not merge at the end of generation\n",
    "\n",
    "merge_model.model = opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=merge_model, tokenizer = tokenizer, device = 0, torch_dtype=torch.float16)\n",
    "generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_input = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ba560",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_prompt = len(tokenizer(your_input)[\"input_ids\"])\n",
    "\n",
    "generator.model.model.starting_tokens = length_of_prompt # You could skip this if you want to fully merge results\n",
    "\n",
    "res = generator(your_input, max_new_tokens = 384, return_full_text = False, pad_token_id=generator.tokenizer.eos_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hugging Face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
